{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e15c005",
      "metadata": {
        "id": "4e15c005"
      },
      "source": [
        "## Author: OMKAR ANANT BARE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc175f88",
      "metadata": {
        "id": "fc175f88",
        "outputId": "353460ea-72b9-40a3-8dc5-a29099036b44"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/omkarbare/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     /Users/omkarbare/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/omkarbare/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/omkarbare/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pandas as pd\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "pd.options.display.max_colwidth=500\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3534b72a",
      "metadata": {
        "id": "3534b72a",
        "outputId": "27302555-c8fc-493b-ec93-2e20fd9c2e9d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Episode</th>\n",
              "      <th>Scene</th>\n",
              "      <th>Scene_info</th>\n",
              "      <th>Character_name</th>\n",
              "      <th>Line</th>\n",
              "      <th>Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1350</td>\n",
              "      <td>1</td>\n",
              "      <td>DESERTED CAR PARK EXT NIGHT</td>\n",
              "      <td>SHIRLEY</td>\n",
              "      <td>Look at ya, not a mark on ya. And you think you're an unlucky man.</td>\n",
              "      <td>FEMALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1350</td>\n",
              "      <td>1</td>\n",
              "      <td>DESERTED CAR PARK EXT NIGHT</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>Shirl...</td>\n",
              "      <td>MALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1350</td>\n",
              "      <td>2</td>\n",
              "      <td>R&amp;R INT NIGHT</td>\n",
              "      <td>JACK</td>\n",
              "      <td>Oi. Where have you been? Huh? What were the texts about?</td>\n",
              "      <td>MALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1350</td>\n",
              "      <td>2</td>\n",
              "      <td>R&amp;R INT NIGHT</td>\n",
              "      <td>RONNIE</td>\n",
              "      <td>Nothing. Nothing. I'll be with you in two minutes yeah?</td>\n",
              "      <td>FEMALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1350</td>\n",
              "      <td>2</td>\n",
              "      <td>R&amp;R INT NIGHT</td>\n",
              "      <td>JACK</td>\n",
              "      <td>Well I've got mates here I wanted to have a chat with them, instead I've been serving behind the bar.</td>\n",
              "      <td>MALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15314</th>\n",
              "      <td>1399</td>\n",
              "      <td>55</td>\n",
              "      <td>SQUARE EXT DAY LIGHT</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>Dad? Okay ... alright, just one drink alright. But that's all. It doesn't mean anything. It's just a drink.</td>\n",
              "      <td>MALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15315</th>\n",
              "      <td>1399</td>\n",
              "      <td>55</td>\n",
              "      <td>SQUARE EXT DAY LIGHT</td>\n",
              "      <td>MAX</td>\n",
              "      <td>Thanks Bradley. Thanks mate... It means the world to me...</td>\n",
              "      <td>MALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15316</th>\n",
              "      <td>1399</td>\n",
              "      <td>55</td>\n",
              "      <td>SQUARE EXT DAY LIGHT</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>You alright...</td>\n",
              "      <td>MALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15317</th>\n",
              "      <td>1399</td>\n",
              "      <td>55</td>\n",
              "      <td>SQUARE EXT DAY LIGHT</td>\n",
              "      <td>MAX</td>\n",
              "      <td>Yeah, yeah, yeah. I'm fine.</td>\n",
              "      <td>MALE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15318</th>\n",
              "      <td>1399</td>\n",
              "      <td>55</td>\n",
              "      <td>SQUARE EXT DAY LIGHT</td>\n",
              "      <td>OTHER</td>\n",
              "      <td>Well, let me know.</td>\n",
              "      <td>MALE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15319 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Episode  Scene                   Scene_info Character_name  \\\n",
              "0         1350      1  DESERTED CAR PARK EXT NIGHT        SHIRLEY   \n",
              "1         1350      1  DESERTED CAR PARK EXT NIGHT          OTHER   \n",
              "2         1350      2                R&R INT NIGHT           JACK   \n",
              "3         1350      2                R&R INT NIGHT         RONNIE   \n",
              "4         1350      2                R&R INT NIGHT           JACK   \n",
              "...        ...    ...                          ...            ...   \n",
              "15314     1399     55         SQUARE EXT DAY LIGHT          OTHER   \n",
              "15315     1399     55         SQUARE EXT DAY LIGHT            MAX   \n",
              "15316     1399     55         SQUARE EXT DAY LIGHT          OTHER   \n",
              "15317     1399     55         SQUARE EXT DAY LIGHT            MAX   \n",
              "15318     1399     55         SQUARE EXT DAY LIGHT          OTHER   \n",
              "\n",
              "                                                                                                              Line  \\\n",
              "0                                               Look at ya, not a mark on ya. And you think you're an unlucky man.   \n",
              "1                                                                                                         Shirl...   \n",
              "2                                                         Oi. Where have you been? Huh? What were the texts about?   \n",
              "3                                                          Nothing. Nothing. I'll be with you in two minutes yeah?   \n",
              "4            Well I've got mates here I wanted to have a chat with them, instead I've been serving behind the bar.   \n",
              "...                                                                                                            ...   \n",
              "15314  Dad? Okay ... alright, just one drink alright. But that's all. It doesn't mean anything. It's just a drink.   \n",
              "15315                                                   Thanks Bradley. Thanks mate... It means the world to me...   \n",
              "15316                                                                                               You alright...   \n",
              "15317                                                                                  Yeah, yeah, yeah. I'm fine.   \n",
              "15318                                                                                           Well, let me know.   \n",
              "\n",
              "       Gender  \n",
              "0      FEMALE  \n",
              "1        MALE  \n",
              "2        MALE  \n",
              "3      FEMALE  \n",
              "4        MALE  \n",
              "...       ...  \n",
              "15314    MALE  \n",
              "15315    MALE  \n",
              "15316    MALE  \n",
              "15317    MALE  \n",
              "15318    MALE  \n",
              "\n",
              "[15319 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load in training data and display in pandas dataframe\n",
        "train_path='training.csv'\n",
        "all_train_data = pd.read_csv(train_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
        "test_path ='test.csv'\n",
        "test_data = pd.read_csv(test_path,  delimiter=\"\\t\", skip_blank_lines = True)\n",
        "\n",
        "# Inspect\n",
        "display(all_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93cd84ee",
      "metadata": {
        "id": "93cd84ee",
        "outputId": "65f1d3e7-259b-42e1-b132-cefff1314b91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1394\n",
            "1254 140\n"
          ]
        }
      ],
      "source": [
        "# Split into training and test data for heldout validation with random samples of 9:1 train/heldout split\n",
        "from random import shuffle, seed\n",
        "\n",
        "seed(0) # set a seed for reproducibility so same split is used each time\n",
        "\n",
        "epsiode_scene_column = all_train_data.Episode.astype(str) + \"-\" + all_train_data.Scene.astype(str)\n",
        "all_train_data['episode_scene'] = epsiode_scene_column\n",
        "episode_scenes = sorted(list(set([x for x in epsiode_scene_column.values]))) # need to sort to ensure same initial order\n",
        "\n",
        "shuffle(episode_scenes)\n",
        "\n",
        "print(len(episode_scenes))\n",
        "episode_split = int(0.9*len(episode_scenes))\n",
        "training_ep_scenes = episode_scenes[:episode_split]\n",
        "test_ep_scenes = episode_scenes[episode_split:]\n",
        "print(len(training_ep_scenes), len(test_ep_scenes))\n",
        "\n",
        "def train_or_heldout_eps(val):\n",
        "    if val in training_ep_scenes:\n",
        "        return \"training\"\n",
        "    return \"heldout\"\n",
        "\n",
        "all_train_data['train_heldout'] = all_train_data['episode_scene'].apply(train_or_heldout_eps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62c6a95c",
      "metadata": {
        "id": "62c6a95c",
        "outputId": "429ea70d-4c14-480b-b2d9-bfc85abbe7bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw Data:  (15319, 8)\n",
            "Train set:  (13638, 8)\n",
            "Validation set:  (1681, 8)\n"
          ]
        }
      ],
      "source": [
        "print('Raw Data: ',np.shape(all_train_data))\n",
        "train_data = all_train_data[all_train_data['train_heldout']=='training']\n",
        "val_data = all_train_data[all_train_data['train_heldout']=='heldout']\n",
        "print('Train set: ',np.shape(train_data))\n",
        "print('Validation set: ',np.shape(val_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2976d29c",
      "metadata": {
        "id": "2976d29c"
      },
      "source": [
        "## adding scene info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efba23e8",
      "metadata": {
        "id": "efba23e8"
      },
      "outputs": [],
      "source": [
        "# Create one document per character\n",
        "def create_character_document_from_dataframe(df, max_line_count):\n",
        "    \"\"\"Returns a dict with the name of the character as key,\n",
        "    their lines joined together as a single string, with end of line _EOL_\n",
        "    markers between them.\n",
        "    \n",
        "    ::max_line_count:: the maximum number of lines to be added per character\n",
        "    \"\"\"\n",
        "    character_docs = {}\n",
        "    character_line_count = {}\n",
        "    scene_information = {}\n",
        "    \n",
        "    for line, name, s_info in zip(df.Line, df.Character_name, df.Scene_info): # added scene info\n",
        "        if not name in character_docs.keys():\n",
        "            \n",
        "            scene_information[name] = {}\n",
        "            for scene_info in df.Scene_info:\n",
        "                scene_info = False\n",
        "            \n",
        "            character_docs[name] = \"\"\n",
        "            character_line_count[name] = 0\n",
        "            \n",
        "        if character_line_count[name] == max_line_count:\n",
        "            continue\n",
        "            \n",
        "        character_docs[name] += str(line)   + \" _EOL_ \"  # adding an end-of-line token\n",
        "        character_line_count[name]+=1\n",
        "        \n",
        "        # Add scene information\n",
        "        scene_information[name][s_info] = True \n",
        "\n",
        "    return character_docs, scene_information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744717af",
      "metadata": {
        "id": "744717af",
        "outputId": "04145f37-d8f3-41db-9e13-2fe589373ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. Characters:  16 \n",
            "\n",
            "SHIRLEY Number of Words:  3848\n",
            "OTHER Number of Words:  3244\n",
            "JACK Number of Words:  4435\n",
            "RONNIE Number of Words:  3442\n",
            "TANYA Number of Words:  3786\n",
            "SEAN Number of Words:  3637\n",
            "ROXY Number of Words:  3838\n",
            "HEATHER Number of Words:  4098\n",
            "MAX Number of Words:  4363\n",
            "IAN Number of Words:  4332\n",
            "JANE Number of Words:  3648\n",
            "STACEY Number of Words:  3913\n",
            "PHIL Number of Words:  3635\n",
            "MINTY Number of Words:  4005\n",
            "CHRISTIAN Number of Words:  3738\n",
            "CLARE Number of Words:  4344\n",
            "\n",
            "Total words: 62306\n"
          ]
        }
      ],
      "source": [
        "# print out the number of words each character has in the training set\n",
        "# only use the first 360 lines of each character\n",
        "train_character_docs, train_scene_info = create_character_document_from_dataframe(train_data, max_line_count=360)\n",
        "                                                                                 \n",
        "print('Num. Characters: ',len(train_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in train_character_docs.keys():\n",
        "    print(name, 'Number of Words: ',len(train_character_docs[name].split()))\n",
        "    total_words += len(train_character_docs[name].split())\n",
        "    \n",
        "print(\"\\nTotal words:\", total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7547151",
      "metadata": {
        "id": "f7547151"
      },
      "outputs": [],
      "source": [
        "# best possible combination of preprocessing switches\n",
        "preprocessing_switches = {\n",
        "        'convert_numbers'      : False,\n",
        "        'separate_punctuation' : True, \n",
        "        'lowercase'            : False,\n",
        "        'remove_punctuation'   : True,\n",
        "        'apply_lemmatization'  : True,\n",
        "        'remove_stopwords'     : True\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1c81b1c",
      "metadata": {
        "id": "a1c81b1c"
      },
      "outputs": [],
      "source": [
        "def pre_process(character_text):\n",
        "    '''\n",
        "    Pre-process all the concatenated lines of a character:\n",
        "    \n",
        "    - convert_numbers: convert any numbers to string 'NUMBER'\n",
        "    - separate_punctuation:  separate punctuations at the end and the beginning\n",
        "    - lowercase : lower-casing\n",
        "    - remove_punctuation: removing any punctuations\n",
        "    - apply_lemmatization: apply lemmatization technoque\n",
        "    - remove_stopwords: removes common stop words in english language\n",
        "    \n",
        "    ::character_text:: a string with all of one character's lines\n",
        "    '''\n",
        "    \n",
        "    # convert any numbers to string 'NUMBER'\n",
        "    if preprocessing_switches['convert_numbers'] == True:\n",
        "        character_text = re.sub('\\d+', 'NUMBER', character_text)\n",
        "     \n",
        "    # simple tokenization on white space\n",
        "    tokens = character_text.split()\n",
        "        \n",
        "    # separates punctuation at beginning and end of strings\n",
        "    if preprocessing_switches['separate_punctuation'] == True:\n",
        "        tokens = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", character_text) # separates punctuation at end of strings\n",
        "        tokens = re.sub(r\"([.,;:!?'\\\"“\\(\\)])(\\w)\", r\"\\1 \\2\", tokens) # separates punctuation at beginning of strings\n",
        "        tokens = re.split(r\"\\s+\", tokens)\n",
        "    \n",
        "    # normalisation - lower casing \n",
        "    if preprocessing_switches['lowercase'] == True:\n",
        "        tokens = [t.lower() for t in tokens]\n",
        "    \n",
        "    # remove puctuations\n",
        "    if preprocessing_switches['remove_punctuation'] == True:\n",
        "        tokens = [word for word in tokens if word.isalpha()] #'isalpha()' method checks if string consists of alphabetic characters only\n",
        "    \n",
        "    # Lemmatizing sentence\n",
        "    if preprocessing_switches['apply_lemmatization'] == True:\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        tokens = [lemmatizer.lemmatize(words_sent) for words_sent in tokens]\n",
        "    \n",
        "    # Removing stop words\n",
        "    processed_data = []\n",
        "    if preprocessing_switches['remove_stopwords'] == True:\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "\n",
        "        for t in tokens:\n",
        "            if t not in stop_words:\n",
        "                processed_data.append(t)\n",
        "        return processed_data\n",
        "    \n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f574d91",
      "metadata": {
        "id": "6f574d91"
      },
      "outputs": [],
      "source": [
        "# create list of pairs of (character name, pre-processed character) \n",
        "training_corpus = [(name, pre_process(doc)) for name, doc in sorted(train_character_docs.items())]\n",
        "train_labels = [name for name, doc in training_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a88b84",
      "metadata": {
        "id": "c5a88b84"
      },
      "outputs": [],
      "source": [
        "def pos_tagging(tokens):\n",
        "    tagged_words = nltk.pos_tag(tokens)\n",
        "    pos_tagged_sentence = [word + \"-\" + tag for word, tag in tagged_words]\n",
        "    return pos_tagged_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "134732f9",
      "metadata": {
        "id": "134732f9"
      },
      "outputs": [],
      "source": [
        "# best setting\n",
        "_WEIGHT_ = \"counts\"  \n",
        "_N_ = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "811531e8",
      "metadata": {
        "id": "811531e8"
      },
      "source": [
        "##  `to_feature_vector_dictionary` function edited for question 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a13b4bc",
      "metadata": {
        "id": "4a13b4bc"
      },
      "outputs": [],
      "source": [
        "def to_feature_vector_dictionary(character_doc, scene_info, extra_features):\n",
        "    '''\n",
        "    function to extract different ngram sequences from tokens and add different weightings to them\n",
        "    '''\n",
        "    # SOLUTION: a method to extract different ngram sequences from tokens\n",
        "    # and different weighting on those counts\n",
        "    \n",
        "    feature_vector_dict =  Counter()  \n",
        "    \n",
        "    # collect the counts for all n in range (1, N)\n",
        "    for n in range(1, _N_+1):\n",
        "        tokens = [\"<s>\"]*(n-1) + character_doc + [\"</s>\"]\n",
        "        \n",
        "        # perform pos tagging\n",
        "        new_tokens = pos_tagging(tokens)\n",
        "            \n",
        "        for i in range(n-1, len(new_tokens)):\n",
        "            raw_ngram = \" \".join(new_tokens[i-(n-1):i+1])\n",
        "            n_gram = \"{}@{}\".format(n, raw_ngram)     \n",
        "            feature_vector_dict[n_gram]+=1\n",
        "        \n",
        "    # binary counts (1 if present)\n",
        "    if _WEIGHT_ == \"binary\":\n",
        "        feature_vector_dict = {x:1 for x in feature_vector_dict.keys()}\n",
        "        \n",
        "    # number of counts\n",
        "    if _WEIGHT_ == \"counts\":\n",
        "        counts = Counter(character_doc)  \n",
        "        feature_vector_dict = dict(counts)   \n",
        "        \n",
        "    # bag-of-words counts\n",
        "    elif _WEIGHT_ == \"weighted\": \n",
        "        feature_vector_dict = {x:feature_vector_dict[x]/(len(character_doc)+1) for x in feature_vector_dict.keys()}\n",
        "    \n",
        "    feature_vector_dict.update(scene_info)\n",
        "    return feature_vector_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d06dab1",
      "metadata": {
        "id": "5d06dab1"
      },
      "outputs": [],
      "source": [
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, scene_info, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary(doc, scene_info[name], []) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc, scene_info[name], []) for name, doc in corpus])\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, train_scene_info, fitting=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5188ae8",
      "metadata": {
        "id": "b5188ae8",
        "outputId": "64dcc992-92ca-4d0d-82b1-70aea8490f69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<16x4476 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 13138 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_feature_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "589b133e",
      "metadata": {
        "id": "589b133e",
        "outputId": "f019b509-bc1f-4c10-82c7-0bb813b45af6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. Characters:  16 \n",
            "\n",
            "TANYA Num of Words:  438\n",
            "MAX Num of Words:  737\n",
            "SEAN Num of Words:  366\n",
            "SHIRLEY Num of Words:  329\n",
            "OTHER Num of Words:  357\n",
            "STACEY Num of Words:  412\n",
            "RONNIE Num of Words:  464\n",
            "JACK Num of Words:  351\n",
            "PHIL Num of Words:  475\n",
            "IAN Num of Words:  508\n",
            "JANE Num of Words:  458\n",
            "ROXY Num of Words:  392\n",
            "HEATHER Num of Words:  411\n",
            "MINTY Num of Words:  470\n",
            "CHRISTIAN Num of Words:  489\n",
            "CLARE Num of Words:  405\n",
            "total words 7062\n"
          ]
        }
      ],
      "source": [
        "# get the validation data- only 40 lines used for each character\n",
        "val_character_docs, val_scene_info = create_character_document_from_dataframe(val_data, max_line_count=40)\n",
        "print('Num. Characters: ',len(val_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in val_character_docs.keys():\n",
        "    print(name, 'Num of Words: ',len(val_character_docs[name].split()))\n",
        "    total_words += len(val_character_docs[name].split())\n",
        "print(\"total words\", total_words)\n",
        "\n",
        "# create list of pairs of (character name, pre-processed character) \n",
        "val_corpus = [(name, pre_process(doc)) for name, doc in sorted(val_character_docs.items())]\n",
        "val_labels = [name for name, doc in val_corpus]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4bf3bd",
      "metadata": {
        "id": "bf4bf3bd"
      },
      "outputs": [],
      "source": [
        "# Just transform the val_feature_matrix, don't fit\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, val_scene_info, fitting=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f070b68",
      "metadata": {
        "id": "3f070b68",
        "outputId": "e3c6ea52-9ec4-44cb-cfe2-30dc12ed6c30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<16x4476 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 2359 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_feature_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9928ef48",
      "metadata": {
        "id": "9928ef48"
      },
      "outputs": [],
      "source": [
        "def compute_cosine_similarity(v1, v2):\n",
        "    \"\"\"Takes a pair of vectors v1 and v2 (1-d arrays e.g. [0, 0.5, 0.5])\n",
        "    returns the cosine similarity between the vectors\n",
        "    \"\"\"\n",
        "    \n",
        "    # compute cosine similarity manually\n",
        "    manual_cosine_similarity = np.dot(v1, v2)  /(norm(v1) * norm(v2))\n",
        "    \n",
        "    return manual_cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5357d8",
      "metadata": {
        "id": "2d5357d8"
      },
      "outputs": [],
      "source": [
        "def compute_IR_evaluation_scores(train_feature_matrix, test_feature_matrix, train_labels, test_labels, display=False):\n",
        "    \"\"\"\n",
        "    Computes an information retrieval based on training data feature matrix and test data feature matrix\n",
        "    returns 4-tuple:\n",
        "    ::mean_rank:: mean of the ranking of the target document in terms of similarity to the query/test document\n",
        "    1 is the best possible score.\n",
        "    ::mean_cosine_similarity:: mean cosine similarity score for the target document vs. the test document of the same class\n",
        "    ::accuracy:: proportion of test documents correctly classified\n",
        "    ::df:: a data frame with all the similarity measures of the test documents vs. train documents\n",
        "    \n",
        "    params:\n",
        "    ::train_feature_matrix:: a numpy matrix N x M shape where N = number of characters M = number of features\n",
        "    ::test_feature_matrix::  a numpy matrix N x M shape where N = number of characters M = number of features\n",
        "    ::train_labels:: a list of character names for the training data in order consistent with train_feature_matrix\n",
        "    ::test_labels:: a list of character names for the test data in order consistent with test_feature_matrix\n",
        "    \"\"\"\n",
        "    rankings = []\n",
        "    all_cosine_similarities = []\n",
        "    pairwise_cosine_similarity = []\n",
        "    pairs = []\n",
        "    correct = 0\n",
        "    for i, target in enumerate(test_labels):\n",
        "        # compare the left out character against the mean\n",
        "        idx = i \n",
        "        fm_1 = test_feature_matrix.toarray()[idx]\n",
        "        all_sims = {}\n",
        "        # print(\"target:\", target)\n",
        "        for j, other in enumerate(train_labels):\n",
        "            fm_2 = train_feature_matrix.toarray()[j]\n",
        "            manual_cosine_similarity = compute_cosine_similarity(fm_1, fm_2)\n",
        "            pairs.append((target, other))\n",
        "            pairwise_cosine_similarity.append(manual_cosine_similarity)\n",
        "            if other == target:\n",
        "                all_cosine_similarities.append(manual_cosine_similarity)\n",
        "            all_sims[other] = manual_cosine_similarity\n",
        "\n",
        "            # print(target, other, manual_cosine_similarity)\n",
        "        sorted_similarities = sorted(all_sims.items(),key=lambda x:x[1],reverse=True)\n",
        "        # print(sorted_similarities)\n",
        "        ranking = {key[0]: rank for rank, key in enumerate(sorted_similarities, 1)}\n",
        "        # print(\"Ranking for target\", ranking[target])\n",
        "        if ranking[target] == 1:\n",
        "            correct += 1\n",
        "        rankings.append(ranking[target])\n",
        "        # print(\"*****\")\n",
        "    mean_rank = np.mean(rankings)\n",
        "    mean_cosine_similarity = np.mean(all_cosine_similarities)\n",
        "    accuracy = correct/len(test_labels)\n",
        "    \n",
        "    if display == True:\n",
        "        print(\"mean rank\", np.mean(rankings))\n",
        "        print(\"mean cosine similarity\", mean_cosine_similarity)\n",
        "        print(correct, \"correct out of\", len(test_labels), \"/ accuracy:\", accuracy)\n",
        "    \n",
        "    # get a dafaframe showing all the similarity scores of training vs test docs\n",
        "    df = pd.DataFrame({'doc1': [x[0] for x in pairs], 'doc2': [x[1] for x in pairs],\n",
        "                       'similarity': pairwise_cosine_similarity})\n",
        "\n",
        "    # display characters which are most similar and least similar\n",
        "    df.loc[[df.similarity.values.argmax(), df.similarity.values.argmin()]]\n",
        "    return (mean_rank, mean_cosine_similarity, accuracy, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2afb9df",
      "metadata": {
        "id": "a2afb9df"
      },
      "outputs": [],
      "source": [
        "def plot_heat_map_similarity(df):\n",
        "    \"\"\"Takes a dataframe with header 'doc1, doc2, similarity'\n",
        "    Plots a heatmap based on the similarity scores.\n",
        "    \"\"\"\n",
        "    test_labels =  sorted(list(set(df.sort_values(['doc1'])['doc1'])))\n",
        "    # add padding 1.0 values to either side\n",
        "    cm = [[1.0,] * (len(test_labels)+2)]\n",
        "    for target in test_labels:\n",
        "        new_row = [1.0]\n",
        "        for x in df.sort_values(['doc1', 'doc2'])[df['doc1']==target]['similarity']:\n",
        "            new_row.append(x)\n",
        "        new_row.append(1.0)\n",
        "        cm.append(new_row)\n",
        "    cm.append([1.0,] * (len(test_labels)+2))\n",
        "    #print(cm)\n",
        "    labels = [\"\"] + test_labels + [\"\"]\n",
        "    fig = plt.figure(figsize=(20,20))\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(cm)\n",
        "    plt.title('Similarity matrix between documents as vectors')\n",
        "    fig.colorbar(cax)\n",
        "    ax.set_xticks(np.arange(len(labels)))\n",
        "    ax.set_yticks(np.arange(len(labels)))\n",
        "    ax.set_xticklabels( labels, rotation=45)\n",
        "    ax.set_yticklabels( labels)\n",
        "\n",
        "    for i in range(len(cm)):\n",
        "        for j in range(len(cm)):\n",
        "\n",
        "            text = ax.text(j, i, round(cm[i][j],3),\n",
        "                           ha=\"center\", va=\"center\", color=\"w\")\n",
        "\n",
        "    plt.xlabel('Training Vector Doc')\n",
        "    plt.ylabel('Test Vector Doc')\n",
        "    #fig.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b2eb4c",
      "metadata": {
        "id": "d9b2eb4c"
      },
      "source": [
        "### results after adding scene information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb34b8e",
      "metadata": {
        "id": "cfb34b8e",
        "outputId": "4da91d1c-e0bb-4ea2-a614-f8e3da0a12b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean rank 1.375\n",
            "mean cosine similarity 0.8049988059079553\n",
            "13 correct out of 16 / accuracy: 0.8125\n"
          ]
        }
      ],
      "source": [
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, \n",
        "                                                                          val_feature_matrix, \n",
        "                                                                          train_labels, \n",
        "                                                                          val_labels,\n",
        "                                                                          display=True)  # set display = True to print results          "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e75c93c2",
      "metadata": {
        "id": "e75c93c2"
      },
      "source": [
        "## Trying different matrix transformation techniques"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32a47c4f",
      "metadata": {
        "id": "32a47c4f"
      },
      "source": [
        "- Trying **TF-IDF** matrix transformation technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af83801a",
      "metadata": {
        "id": "af83801a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "corpusVectorizer = DictVectorizer()   # corpusVectorizor which will just produce sparse vectors from feature dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "tfidf = TfidfTransformer()\n",
        "\n",
        "\n",
        "def create_document_matrix_from_corpus(corpus, scene_info, fitting=False):\n",
        "    \"\"\"Method which fits different vectorizers\n",
        "    on data and returns a matrix.\n",
        "    \n",
        "    Currently just does simple conversion to matrix by vectorizing the dictionary. Improve this for Q3.\n",
        "    \n",
        "    ::corpus:: a list of (class_label, document) pairs.\n",
        "    ::fitting:: a boolean indicating whether to fit/train the vectorizers (should be true on training data)\n",
        "    \"\"\"\n",
        "    \n",
        "    # uses the global variable of the corpus Vectorizer to improve things\n",
        "    if fitting:\n",
        "        corpusVectorizer.fit([to_feature_vector_dictionary(doc, scene_info[name], []) for name, doc in corpus])\n",
        "    doc_feature_matrix = corpusVectorizer.transform([to_feature_vector_dictionary(doc, scene_info[name], []) for name, doc in corpus])\n",
        "    \n",
        "    # converts a matrix to tf-idf representation\n",
        "    doc_feature_matrix = tfidf.fit_transform(doc_feature_matrix)\n",
        "    \n",
        "    #training_feature_matrix[0].toarray()\n",
        "    return doc_feature_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09f39722",
      "metadata": {
        "id": "09f39722"
      },
      "source": [
        "#### results after improving vectorization method "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6ba71de",
      "metadata": {
        "id": "f6ba71de",
        "outputId": "54a90653-fca7-4331-ca42-2c4c388e57a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results on validation set:\n",
            "mean rank 1.125\n",
            "mean cosine similarity 0.576010719865794\n",
            "15 correct out of 16 / accuracy: 0.9375\n"
          ]
        }
      ],
      "source": [
        "# get the training data - only 360 lines used for each character\n",
        "train_character_docs, train_scene_info = create_character_document_from_dataframe(train_data, max_line_count=360)\n",
        "training_corpus = [(name, pre_process(doc)) for name, doc in sorted(train_character_docs.items())]\n",
        "train_labels = [name for name, doc in training_corpus]\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, train_scene_info, fitting=True)\n",
        "\n",
        "# get the validation data- only 40 lines used for each character\n",
        "val_character_docs, val_scene_info = create_character_document_from_dataframe(val_data, max_line_count=40)\n",
        "# create list of pairs of (character name, pre-processed character) \n",
        "val_corpus = [(name, pre_process(doc)) for name, doc in sorted(val_character_docs.items())]\n",
        "val_labels = [name for name, doc in val_corpus]\n",
        "\n",
        "# Just transform the val_feature_matrix, don't fit\n",
        "val_feature_matrix = create_document_matrix_from_corpus(val_corpus, val_scene_info, fitting=False)\n",
        "\n",
        "print('\\nResults on validation set:')\n",
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, \n",
        "                                                                          val_feature_matrix, \n",
        "                                                                          train_labels, \n",
        "                                                                          val_labels, \n",
        "                                                                          display=True) # set display = True to print results  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b00398",
      "metadata": {
        "id": "f0b00398"
      },
      "source": [
        "## best system trained on all of the training data (using the first 400 lines per character) and final testing done on the test file (using the first 40 lines per character)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de7275fd",
      "metadata": {
        "id": "de7275fd"
      },
      "outputs": [],
      "source": [
        "# best possible combination of preprocessing switches\n",
        "preprocessing_switches = {\n",
        "        'convert_numbers'      : False,\n",
        "        'separate_punctuation' : True, \n",
        "        'lowercase'            : False,\n",
        "        'remove_punctuation'   : True,\n",
        "        'apply_lemmatization'  : True,\n",
        "        'remove_stopwords'     : True\n",
        "    }\n",
        "# best setting\n",
        "_WEIGHT_ = \"counts\"  \n",
        "_N_ = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac5792c0",
      "metadata": {
        "id": "ac5792c0",
        "outputId": "5c7dce0a-802b-4ed6-e582-8ef20e5c9fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. Characters:  16 \n",
            "\n",
            "SHIRLEY Number of Words:  4233\n",
            "OTHER Number of Words:  3606\n",
            "JACK Number of Words:  4786\n",
            "RONNIE Number of Words:  3872\n",
            "TANYA Number of Words:  4214\n",
            "SEAN Number of Words:  4026\n",
            "ROXY Number of Words:  4200\n",
            "HEATHER Number of Words:  4504\n",
            "MAX Number of Words:  5107\n",
            "IAN Number of Words:  4863\n",
            "JANE Number of Words:  4117\n",
            "STACEY Number of Words:  4325\n",
            "PHIL Number of Words:  4103\n",
            "MINTY Number of Words:  4391\n",
            "CHRISTIAN Number of Words:  4250\n",
            "CLARE Number of Words:  4844\n",
            "total words 69441\n",
            "Num. Characters:  16 \n",
            "\n",
            "SHIRLEY Number of Words:  373\n",
            "OTHER Number of Words:  453\n",
            "HEATHER Number of Words:  451\n",
            "PHIL Number of Words:  406\n",
            "SEAN Number of Words:  466\n",
            "TANYA Number of Words:  465\n",
            "MAX Number of Words:  494\n",
            "JACK Number of Words:  412\n",
            "IAN Number of Words:  509\n",
            "JANE Number of Words:  414\n",
            "STACEY Number of Words:  634\n",
            "ROXY Number of Words:  392\n",
            "RONNIE Number of Words:  390\n",
            "CHRISTIAN Number of Words:  629\n",
            "MINTY Number of Words:  428\n",
            "CLARE Number of Words:  368\n",
            "total words 7284\n",
            "mean rank 1.75\n",
            "mean cosine similarity 0.6039444980729507\n",
            "14 correct out of 16 / accuracy: 0.875\n"
          ]
        }
      ],
      "source": [
        "# redo on all training data with the first 400 character lines used\n",
        "train_character_docs, train_scene_info = create_character_document_from_dataframe(all_train_data, max_line_count=400)\n",
        "print('Num. Characters: ',len(train_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in train_character_docs.keys():\n",
        "    print(name, 'Number of Words: ',len(train_character_docs[name].split()))\n",
        "    total_words += len(train_character_docs[name].split())\n",
        "print(\"total words\", total_words)\n",
        "\n",
        "training_corpus = [(name, pre_process(doc)) for name, doc in train_character_docs.items()]\n",
        "train_labels = [name for name, doc in training_corpus]\n",
        "\n",
        "corpusVectorizer = DictVectorizer()   # initialize a corpusVectorizor which will output sparse vectors from dicts\n",
        "# Any matrix transformers (e.g. tf-idf transformers) should be initialized here\n",
        "tfidf = TfidfTransformer()\n",
        "\n",
        "\n",
        "training_feature_matrix = create_document_matrix_from_corpus(training_corpus, train_scene_info, fitting=True)\n",
        "\n",
        "# get the test data using 40 lines per character\n",
        "test_character_docs, val_scene_info = create_character_document_from_dataframe(test_data, max_line_count=40)\n",
        "print('Num. Characters: ',len(test_character_docs.keys()),\"\\n\")\n",
        "total_words = 0\n",
        "for name in test_character_docs.keys():\n",
        "    print(name, 'Number of Words: ',len(test_character_docs[name].split()))\n",
        "    total_words += len(test_character_docs[name].split())\n",
        "print(\"total words\", total_words)\n",
        "\n",
        "# create list of pairs of (character name, pre-processed character) \n",
        "test_corpus = [(name, pre_process(doc)) for name, doc in test_character_docs.items()]\n",
        "test_labels = [name for name, doc in test_corpus]\n",
        "\n",
        "\n",
        "# Just transform the val_feature_matrix, don't fit\n",
        "test_feature_matrix = create_document_matrix_from_corpus(test_corpus, val_scene_info, fitting=False)\n",
        "\n",
        "\n",
        "mean_rank, mean_cosine_simliarity, acc, df = compute_IR_evaluation_scores(training_feature_matrix, test_feature_matrix, train_labels, test_labels, display=True)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}